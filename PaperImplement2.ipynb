{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tf-Xqh6Jm0Ms",
    "outputId": "cb4a8a8f-5654-457e-93a5-e675f4f7aa50"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\sayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\sayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\sayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\sayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\sayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\sayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\sayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\sayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\sayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\sayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\sayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\sayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\omw.zip.\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\sayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\omw-1.4.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\sayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\sayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2021.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\sayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet31.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\sayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\sayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\sayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\sayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\sayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\sayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://raw.githubusercontent.com/SenticNet/personality-detection/master/essays.csv'\n",
    "res = requests.get(url, allow_redirects=True)\n",
    "with open('essays.csv','wb') as file:\n",
    "    file.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 8249
    },
    "id": "jUjEOoamG_ze",
    "outputId": "029c5e52-8127-47c4-eaa1-3da3b595cb35"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997_504851.txt</td>\n",
       "      <td>Well, right now I just woke up from a mid-day ...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997_605191.txt</td>\n",
       "      <td>Well, here we go with the stream of consciousn...</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997_687252.txt</td>\n",
       "      <td>An open keyboard and buttons to push. The thin...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997_568848.txt</td>\n",
       "      <td>I can't believe it!  It's really happening!  M...</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997_688160.txt</td>\n",
       "      <td>Well, here I go with the good old stream of co...</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2462</th>\n",
       "      <td>2004_493.txt</td>\n",
       "      <td>I'm home. wanted to go to bed but remembe...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463</th>\n",
       "      <td>2004_494.txt</td>\n",
       "      <td>Stream of consiousnesssskdj. How do you s...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>2004_497.txt</td>\n",
       "      <td>It is Wednesday, December 8th and a lot has be...</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>2004_498.txt</td>\n",
       "      <td>Man this week has been hellish. Anyways, now i...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>2004_499.txt</td>\n",
       "      <td>I have just gotten off the phone with brady. I...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2467 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              #AUTHID                                               TEXT cEXT  \\\n",
       "0     1997_504851.txt  Well, right now I just woke up from a mid-day ...    n   \n",
       "1     1997_605191.txt  Well, here we go with the stream of consciousn...    n   \n",
       "2     1997_687252.txt  An open keyboard and buttons to push. The thin...    n   \n",
       "3     1997_568848.txt  I can't believe it!  It's really happening!  M...    y   \n",
       "4     1997_688160.txt  Well, here I go with the good old stream of co...    y   \n",
       "...               ...                                                ...  ...   \n",
       "2462     2004_493.txt       I'm home. wanted to go to bed but remembe...    n   \n",
       "2463     2004_494.txt       Stream of consiousnesssskdj. How do you s...    y   \n",
       "2464     2004_497.txt  It is Wednesday, December 8th and a lot has be...    n   \n",
       "2465     2004_498.txt  Man this week has been hellish. Anyways, now i...    n   \n",
       "2466     2004_499.txt  I have just gotten off the phone with brady. I...    n   \n",
       "\n",
       "     cNEU cAGR cCON cOPN  \n",
       "0       y    y    n    y  \n",
       "1       n    y    n    n  \n",
       "2       y    n    y    y  \n",
       "3       n    y    y    n  \n",
       "4       n    y    n    y  \n",
       "...   ...  ...  ...  ...  \n",
       "2462    y    n    y    n  \n",
       "2463    y    n    n    y  \n",
       "2464    n    y    n    n  \n",
       "2465    y    n    n    y  \n",
       "2466    y    y    n    y  \n",
       "\n",
       "[2467 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"essays.csv\", encoding='cp1252')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#AUTHID                                      1999_973407.txt\n",
       "TEXT       For the past few days and now I have been thin...\n",
       "cEXT                                                       y\n",
       "cNEU                                                       n\n",
       "cAGR                                                       y\n",
       "cCON                                                       n\n",
       "cOPN                                                       n\n",
       "Name: 1016, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1016]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "nEctgfKYpNnz"
   },
   "outputs": [],
   "source": [
    "letterToNumber = {\n",
    "    \"y\":1,\n",
    "    \"n\":0\n",
    "}\n",
    "try:\n",
    "    for i,j in df.iterrows():\n",
    "        if j[\"cEXT\"] in letterToNumber.keys() or j[\"cNEU\"] in letterToNumber.keys() or j[\"cAGR\"] in letterToNumber.keys() or j[\"cOPN\"] in letterToNumber.keys() or  j[\"cCON\"] in letterToNumber.keys():\n",
    "            j[\"cEXT\"]=letterToNumber[j[\"cEXT\"]]\n",
    "            j[\"cNEU\"]=letterToNumber[j[\"cNEU\"]]\n",
    "            j[\"cAGR\"]=letterToNumber[j[\"cAGR\"]]\n",
    "            j[\"cOPN\"]=letterToNumber[j[\"cOPN\"]]\n",
    "            j[\"cCON\"]=letterToNumber[j[\"cCON\"]]\n",
    "except:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "0mCo-WECIrTM",
    "outputId": "e4d4e133-d853-4b1b-a6d3-9d8ee7a544a4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>Charged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>march</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>august</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ago</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vie</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Words  anger  anticipation  disgust  fear  joy  negative  positive  \\\n",
       "0   march      0             0        0     0    0         0         1   \n",
       "1  august      0             0        0     0    0         0         1   \n",
       "2     ago      0             0        0     0    0         0         0   \n",
       "3     mar      0             0        0     0    0         1         0   \n",
       "4     vie      0             0        0     0    0         0         0   \n",
       "\n",
       "   sadness  surprise  trust  Charged  \n",
       "0        0         0      0        1  \n",
       "1        0         0      0        1  \n",
       "2        0         0      0        0  \n",
       "3        0         0      0        1  \n",
       "4        0         0      0        0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoLex = pd.read_csv('Emotion_Lexicon.csv')\n",
    "count = 0\n",
    "emoWords=[]\n",
    "\n",
    "for word in emoLex[\"Words\"]:\n",
    "  if emoLex[\"Charged\"][count]==1:\n",
    "    emoWords.append(word)\n",
    "\n",
    "emoWordSet = set(emoWords)\n",
    "\n",
    "emoLex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZbSxE6mRGj3T",
    "outputId": "37d57f1e-3e52-4467-c0ef-04086c8d0297"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [well, right, woke, midday, nap, sort, weird, ...\n",
       "1       [well, go, stream, consciousness, essay, used,...\n",
       "2       [open, keyboard, buttons, push, thing, finally...\n",
       "3       [ca, nt, believe, really, happening, pulse, ra...\n",
       "4       [well, go, good, old, stream, consciousness, a...\n",
       "                              ...                        \n",
       "2462    [home, wanted, go, bed, remembered, psychology...\n",
       "2463    [stream, consiousnesssskdj, spell, fuck, know,...\n",
       "2464    [wednesday, december, lot, going, semester, tr...\n",
       "2465    [man, week, hellish, anyways, time, minute, wr...\n",
       "2466    [gotten, phone, brady, trying, decide, exacly,...\n",
       "Name: DOC, Length: 2467, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text cleaning and tokenizing pipeline\n",
    "import string\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "df['DOC']=''\n",
    "\n",
    "all_words=[]\n",
    "\n",
    "\n",
    "for i,j in df.iterrows():\n",
    "  doc = []\n",
    "  words = word_tokenize(j[\"TEXT\"])\n",
    "  wordSet = set(words)\n",
    "  #check if it has any charged wordswembeddings\n",
    "  if(emoWordSet & wordSet):\n",
    "    #convert to lowercase\n",
    "    words = [word.lower() for word in words]\n",
    "    #remove punctuation from each word\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    words = [word.translate(table) for word in words]\n",
    "    #remove anything that isn't alpha\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    #remove stop words\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "    for word in words:\n",
    "      all_words.append(word)\n",
    "      doc.append(word)\n",
    "  j[\"DOC\"]=doc\n",
    "    \n",
    "df[\"DOC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R5glspoRaQWH",
    "outputId": "91487bd5-19e2-42c8-8e65-30cba997626c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "2462    0\n",
       "2463    1\n",
       "2464    0\n",
       "2465    0\n",
       "2466    0\n",
       "Name: cEXT, Length: 2467, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cEXT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7YoFRI_2UsDy"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "max_len = 200\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_words)\n",
    "\n",
    "df[\"PADDED\"]=''\n",
    "sequence = tokenizer.texts_to_sequences(df[\"DOC\"])\n",
    "padded = pad_sequences(sequences=sequence,maxlen=max_len,padding='post')\n",
    "\n",
    "for i,j in df.iterrows():\n",
    "    if i<=1015:\n",
    "        j[\"PADDED\"]=padded[i]\n",
    "    else:\n",
    "        j[\"PADDED\"]=padded[i-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AYNprw-bkOXz",
    "outputId": "0b75ce15-9e90-41d5-f87c-138eebc6d65b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [342, 20, 1, 690, 908, 7, 420, 4944, 283, 37, ...\n",
       "1       [94, 317, 464, 32, 786, 199, 106, 28, 16, 1, 5...\n",
       "2       [6248, 400, 286, 10, 30, 751, 11103, 231, 4120...\n",
       "3       [20, 1, 135, 3, 1025, 7852, 4951, 2, 292, 2, 3...\n",
       "4       [1158, 9, 52, 14, 562, 143, 6, 331, 1476, 30, ...\n",
       "                              ...                        \n",
       "2462    [37, 168, 10, 288, 1395, 210, 163, 641, 737, 9...\n",
       "2463    [9, 30147, 30148, 498, 30149, 30150, 219, 498,...\n",
       "2464    [1244, 3314, 32, 9, 277, 125, 275, 277, 644, 5...\n",
       "2465    [45, 775, 29, 6414, 88, 23, 51, 2232, 1280, 39...\n",
       "2466    [7765, 53, 2202, 101, 48, 30180, 51, 48, 13200...\n",
       "Name: PADDED, Length: 2466, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_pickle('/content/drive/My Drive/NLP/Cleaned')\n",
    "np.save('/content/drive/My Drive/NLP/word_index',tokenizer.word_index,allow_pickle=True)\n",
    "np.save('/content/drive/My Drive/NLP/allwords',all_words,allow_pickle=True)\n",
    "#df = pd.read_pickle('/content/drive/My Drive/NLP/Cleaned')\n",
    "\n",
    "df[\"PADDED\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0sd1jkBHp-L1",
    "outputId": "2fb9638a-12c0-4c1b-93c9-fec573d697fe"
   },
   "outputs": [],
   "source": [
    "!wget https://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzip the file manually on windows\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.name != 'nt':\n",
    "    !unzip glove*.zip\n",
    "else:\n",
    "    print(\"Unzip the file manually on windows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yem6MUYBUgjw",
    "outputId": "81562a71-dfbe-4e2d-e0e3-eae0dd86f93c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_index = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "  values = line.split()\n",
    "  word = values[0]\n",
    "  coefs = np.asarray(values[1:],dtype='float32')\n",
    "  embedding_index[word]=coefs\n",
    "f.close()\n",
    "len(embedding_index)\n",
    "len(coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Va0Gk75kUqDD"
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "vocab = len(word_index)+1\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HCUdLON6eedR",
    "outputId": "94b5e7d3-bb07-4fea-ed77-aae34a4a0b6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1652, 200) (815, 200) (1652,) (815,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X= padded\n",
    "y=df[\"cEXT\"]\n",
    "X_train,X_test,y_train,y_test =  train_test_split(X,y,test_size=0.33,random_state=42)\n",
    "\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "eBnMFJLGYnuo",
    "outputId": "2d893607-aeae-42a4-b8d2-e60147918d83"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "      <th>DOC</th>\n",
       "      <th>PADDED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997_504851.txt</td>\n",
       "      <td>Well, right now I just woke up from a mid-day ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[well, right, woke, midday, nap, sort, weird, ...</td>\n",
       "      <td>[342, 20, 1, 690, 902, 7, 420, 4946, 283, 37, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997_605191.txt</td>\n",
       "      <td>Well, here we go with the stream of consciousn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[well, go, stream, consciousness, essay, used,...</td>\n",
       "      <td>[94, 317, 464, 32, 786, 199, 106, 28, 16, 1, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997_687252.txt</td>\n",
       "      <td>An open keyboard and buttons to push. The thin...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[open, keyboard, buttons, push, thing, finally...</td>\n",
       "      <td>[6250, 400, 286, 10, 30, 751, 11112, 231, 4123...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997_568848.txt</td>\n",
       "      <td>I can't believe it!  It's really happening!  M...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[ca, nt, believe, really, happening, pulse, ra...</td>\n",
       "      <td>[20, 1, 135, 3, 1025, 7854, 4953, 2, 292, 2, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997_688160.txt</td>\n",
       "      <td>Well, here I go with the good old stream of co...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[well, go, good, old, stream, consciousness, a...</td>\n",
       "      <td>[1160, 9, 52, 14, 563, 143, 6, 331, 1476, 30, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2462</th>\n",
       "      <td>2004_493.txt</td>\n",
       "      <td>I'm home. wanted to go to bed but remembe...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[home, wanted, go, bed, remembered, psychology...</td>\n",
       "      <td>[1819, 1286, 3099, 869, 412, 178, 1668, 2569, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463</th>\n",
       "      <td>2004_494.txt</td>\n",
       "      <td>Stream of consiousnesssskdj. How do you s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[stream, consiousnesssskdj, spell, fuck, know,...</td>\n",
       "      <td>[37, 168, 10, 288, 1395, 210, 163, 641, 737, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>2004_497.txt</td>\n",
       "      <td>It is Wednesday, December 8th and a lot has be...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[wednesday, december, lot, going, semester, tr...</td>\n",
       "      <td>[9, 30181, 30182, 498, 30183, 30184, 219, 498,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>2004_498.txt</td>\n",
       "      <td>Man this week has been hellish. Anyways, now i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[man, week, hellish, anyways, time, minute, wr...</td>\n",
       "      <td>[1244, 3317, 32, 9, 277, 125, 275, 277, 644, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>2004_499.txt</td>\n",
       "      <td>I have just gotten off the phone with brady. I...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[gotten, phone, brady, trying, decide, exacly,...</td>\n",
       "      <td>[45, 775, 29, 6416, 88, 23, 51, 2232, 1280, 39...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2467 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              #AUTHID                                               TEXT cEXT  \\\n",
       "0     1997_504851.txt  Well, right now I just woke up from a mid-day ...    0   \n",
       "1     1997_605191.txt  Well, here we go with the stream of consciousn...    0   \n",
       "2     1997_687252.txt  An open keyboard and buttons to push. The thin...    0   \n",
       "3     1997_568848.txt  I can't believe it!  It's really happening!  M...    1   \n",
       "4     1997_688160.txt  Well, here I go with the good old stream of co...    1   \n",
       "...               ...                                                ...  ...   \n",
       "2462     2004_493.txt       I'm home. wanted to go to bed but remembe...    0   \n",
       "2463     2004_494.txt       Stream of consiousnesssskdj. How do you s...    1   \n",
       "2464     2004_497.txt  It is Wednesday, December 8th and a lot has be...    0   \n",
       "2465     2004_498.txt  Man this week has been hellish. Anyways, now i...    0   \n",
       "2466     2004_499.txt  I have just gotten off the phone with brady. I...    0   \n",
       "\n",
       "     cNEU cAGR cCON cOPN                                                DOC  \\\n",
       "0       1    1    0    1  [well, right, woke, midday, nap, sort, weird, ...   \n",
       "1       0    1    0    0  [well, go, stream, consciousness, essay, used,...   \n",
       "2       1    0    1    1  [open, keyboard, buttons, push, thing, finally...   \n",
       "3       0    1    1    0  [ca, nt, believe, really, happening, pulse, ra...   \n",
       "4       0    1    0    1  [well, go, good, old, stream, consciousness, a...   \n",
       "...   ...  ...  ...  ...                                                ...   \n",
       "2462    1    0    1    0  [home, wanted, go, bed, remembered, psychology...   \n",
       "2463    1    0    0    1  [stream, consiousnesssskdj, spell, fuck, know,...   \n",
       "2464    0    1    0    0  [wednesday, december, lot, going, semester, tr...   \n",
       "2465    1    0    0    1  [man, week, hellish, anyways, time, minute, wr...   \n",
       "2466    1    1    0    1  [gotten, phone, brady, trying, decide, exacly,...   \n",
       "\n",
       "                                                 PADDED  \n",
       "0     [342, 20, 1, 690, 902, 7, 420, 4946, 283, 37, ...  \n",
       "1     [94, 317, 464, 32, 786, 199, 106, 28, 16, 1, 5...  \n",
       "2     [6250, 400, 286, 10, 30, 751, 11112, 231, 4123...  \n",
       "3     [20, 1, 135, 3, 1025, 7854, 4953, 2, 292, 2, 3...  \n",
       "4     [1160, 9, 52, 14, 563, 143, 6, 331, 1476, 30, ...  \n",
       "...                                                 ...  \n",
       "2462  [1819, 1286, 3099, 869, 412, 178, 1668, 2569, ...  \n",
       "2463  [37, 168, 10, 288, 1395, 210, 163, 641, 737, 9...  \n",
       "2464  [9, 30181, 30182, 498, 30183, 30184, 219, 498,...  \n",
       "2465  [1244, 3317, 32, 9, 277, 125, 275, 277, 644, 5...  \n",
       "2466  [45, 775, 29, 6416, 88, 23, 51, 2232, 1280, 39...  \n",
       "\n",
       "[2467 rows x 9 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "zf3QsprJS04S"
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "y_train = np_utils.to_categorical(y_train,num_classes=2)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "jK93kpWWe3cU"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv2D,Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import regularizers\n",
    "\n",
    "def define_model(length,vocab):\n",
    "  #channel for 1-gram\n",
    "  input1 = Input(shape=(length,))\n",
    "  embed1=Embedding(vocab,EMBEDDING_DIM,weights=[embedding_matrix],input_length=length,trainable=False)(input1)\n",
    "  conv1 = Conv1D(filters=32,kernel_size=1,activation='relu')(embed1)\n",
    "  drop1=Dropout(0.5)(conv1)\n",
    "  pool1=MaxPooling1D(pool_size=2)(drop1)\n",
    "  flat1=Flatten()(pool1)\n",
    "\n",
    "  #Channel for 2-gram\n",
    "  input2 = Input(shape=(length,))\n",
    "  embed2=Embedding(vocab,EMBEDDING_DIM,weights=[embedding_matrix],input_length=length,trainable=False)(input2)\n",
    "  conv2 = Conv1D(filters=32,kernel_size=2,activation='relu')(embed2)\n",
    "  drop2=Dropout(0.5)(conv2)\n",
    "  pool2=MaxPooling1D(pool_size=2)(drop2)\n",
    "  flat2=Flatten()(pool2)\n",
    "\n",
    "  #channel for 3-gram\n",
    "  input3 = Input(shape=(length,))\n",
    "  embed3=Embedding(vocab,EMBEDDING_DIM,weights=[embedding_matrix],input_length=length,trainable=False)(input3)\n",
    "  conv3 = Conv1D(filters=32,kernel_size=3,activation='relu')(embed3)\n",
    "  drop3=Dropout(0.5)(conv3)\n",
    "  pool3=MaxPooling1D(pool_size=2)(drop3)\n",
    "  flat3=Flatten()(pool3)\n",
    "\n",
    "  #merge\n",
    "  merged = concatenate([flat1,flat2,flat3])\n",
    "\n",
    "  #MLP\n",
    "  dense1 = Dense(256, activation='relu')(merged)\n",
    "  dense2 = Dense(128, activation='relu',kernel_regularizer=regularizers.l2(0.01))(dense1)\n",
    "  drop = Dropout(0.3)(dense2)\n",
    "  dense3 = Dense(64, activation='relu')(drop)\n",
    "  outputs = Dense(2, activation='softmax')(dense3)\n",
    "  model = Model(inputs=[input1, input2, input3], outputs=outputs)\n",
    "  # compile\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  # summarize\n",
    "  print(model.summary())\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CyA8KXO9ikle",
    "outputId": "7bb5e970-aca1-41a6-8c89-e930e04ade8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 200, 100)     3022200     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 100)     3022200     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 200, 100)     3022200     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 200, 32)      3232        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 199, 32)      6432        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 198, 32)      9632        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 200, 32)      0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 199, 32)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 198, 32)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 100, 32)      0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 99, 32)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 99, 32)       0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 3200)         0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3168)         0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 3168)         0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 9536)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          2441472     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          32896       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           8256        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            130         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 11,568,650\n",
      "Trainable params: 2,502,050\n",
      "Non-trainable params: 9,066,600\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 1.6467 - accuracy: 0.5079\n",
      "Epoch 2/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.9781 - accuracy: 0.5442\n",
      "Epoch 3/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.8149 - accuracy: 0.6380\n",
      "Epoch 4/50\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.5979 - accuracy: 0.7912\n",
      "Epoch 5/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.4181 - accuracy: 0.8547\n",
      "Epoch 6/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.3129 - accuracy: 0.8910\n",
      "Epoch 7/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.2338 - accuracy: 0.9280\n",
      "Epoch 8/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.1853 - accuracy: 0.9455\n",
      "Epoch 9/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.1646 - accuracy: 0.9473\n",
      "Epoch 10/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.1387 - accuracy: 0.9528\n",
      "Epoch 11/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.1199 - accuracy: 0.9619\n",
      "Epoch 12/50\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.1114 - accuracy: 0.9661\n",
      "Epoch 13/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.1196 - accuracy: 0.9631\n",
      "Epoch 14/50\n",
      "104/104 [==============================] - 2s 17ms/step - loss: 0.1228 - accuracy: 0.9588\n",
      "Epoch 15/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0763 - accuracy: 0.9746\n",
      "Epoch 16/50\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0764 - accuracy: 0.9800\n",
      "Epoch 17/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0958 - accuracy: 0.9691\n",
      "Epoch 18/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0826 - accuracy: 0.9770\n",
      "Epoch 19/50\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0679 - accuracy: 0.9794\n",
      "Epoch 20/50\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0636 - accuracy: 0.9843\n",
      "Epoch 21/50\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0922 - accuracy: 0.9734\n",
      "Epoch 22/50\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0753 - accuracy: 0.9788\n",
      "Epoch 23/50\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0546 - accuracy: 0.9873\n",
      "Epoch 24/50\n",
      "104/104 [==============================] - 2s 17ms/step - loss: 0.0469 - accuracy: 0.9867\n",
      "Epoch 25/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0661 - accuracy: 0.9800\n",
      "Epoch 26/50\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0481 - accuracy: 0.9867\n",
      "Epoch 27/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0589 - accuracy: 0.9824\n",
      "Epoch 28/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0478 - accuracy: 0.9855\n",
      "Epoch 29/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0409 - accuracy: 0.9879\n",
      "Epoch 30/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0565 - accuracy: 0.9837\n",
      "Epoch 31/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0512 - accuracy: 0.9861\n",
      "Epoch 32/50\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0472 - accuracy: 0.9861\n",
      "Epoch 33/50\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0513 - accuracy: 0.9867\n",
      "Epoch 34/50\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0409 - accuracy: 0.9897\n",
      "Epoch 35/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0440 - accuracy: 0.9879 0s - loss: 0.0435 - accura\n",
      "Epoch 36/50\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0517 - accuracy: 0.9861\n",
      "Epoch 37/50\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0378 - accuracy: 0.9903\n",
      "Epoch 38/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0443 - accuracy: 0.9897\n",
      "Epoch 39/50\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0455 - accuracy: 0.9855\n",
      "Epoch 40/50\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0508 - accuracy: 0.9861\n",
      "Epoch 41/50\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0319 - accuracy: 0.9933\n",
      "Epoch 42/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0434 - accuracy: 0.9891\n",
      "Epoch 43/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0384 - accuracy: 0.9903\n",
      "Epoch 44/50\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0376 - accuracy: 0.9879\n",
      "Epoch 45/50\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0426 - accuracy: 0.9861 0s - loss: 0.042\n",
      "Epoch 46/50\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0304 - accuracy: 0.9915\n",
      "Epoch 47/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0437 - accuracy: 0.9885\n",
      "Epoch 48/50\n",
      "104/104 [==============================] - 2s 17ms/step - loss: 0.0422 - accuracy: 0.9885\n",
      "Epoch 49/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0277 - accuracy: 0.9933\n",
      "Epoch 50/50\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0261 - accuracy: 0.9927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2337f7e7cc8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = define_model(length=max_len,vocab=vocab)\n",
    "model.fit([X_train,X_train,X_train],y_train,epochs=50,batch_size=16,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-JR4yVExVy9C",
    "outputId": "a6142c8a-e9a6-496a-e0ac-30bef75d8e78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5153374075889587\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate([X_test,X_test,X_test],y_test, verbose=0)\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PaperImplement2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
